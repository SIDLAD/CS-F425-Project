{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73WcCtOSc-t9",
        "outputId": "71db5f54-414a-4e69-8b58-0746ea36643d"
      },
      "outputs": [],
      "source": [
        "RunningInColab = 'google.colab' in str(get_ipython())\n",
        "if RunningInColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lbRunumRL6oh"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "from math import ceil\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hjZxNkGLJmyK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "TARGET_LENGTH_SECONDS = 4\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 3e-4\n",
        "DROPOUT_PROB = 0.4\n",
        "DROPOUT_PROB_2D = 0.2\n",
        "\n",
        "# FREQ_MASK_PARAM = 15\n",
        "# TIME_MASK_PARAM = 25\n",
        "\n",
        "NUM_SAMPLES = TARGET_LENGTH_SECONDS * TARGET_SAMPLE_RATE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "transformations = [\n",
        "    transforms.MelSpectrogram(\n",
        "            sample_rate=TARGET_SAMPLE_RATE,\n",
        "            n_fft = 1024,\n",
        "            hop_length = 512,\n",
        "            n_mels = 64\n",
        "        ),\n",
        "    # transforms.TimeMasking(time_mask_param=TIME_MASK_PARAM),\n",
        "    # transforms.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VjgT3fgxcleX"
      },
      "outputs": [],
      "source": [
        "class_mapping = {}\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               data_dir,\n",
        "               transformations = transformations,\n",
        "               target_sample_rate = TARGET_SAMPLE_RATE,\n",
        "               num_samples = NUM_SAMPLES,\n",
        "               device = device\n",
        "               ):\n",
        "    self.data_dir = data_dir\n",
        "    self.classes = sorted(os.listdir(data_dir))\n",
        "    self.file_paths = []\n",
        "    self.targets = []\n",
        "    self.transformations = transformations\n",
        "    self.target_sample_rate = target_sample_rate\n",
        "    self.num_samples = num_samples\n",
        "    self.device = device\n",
        "\n",
        "    createMapping = False\n",
        "    if(len(class_mapping) == 0):createMapping = True\n",
        "\n",
        "    for i,class_name in enumerate(self.classes):\n",
        "      class_dir = os.path.join(data_dir,class_name)\n",
        "      if(createMapping):class_mapping[class_name] = i\n",
        "\n",
        "      for filename in os.listdir(class_dir):\n",
        "        filepath = os.path.join(class_dir,filename)\n",
        "        self.file_paths.append(filepath)\n",
        "        self.targets.append(class_mapping[class_name])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_paths)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    audio_path = self.file_paths[idx]\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    waveform = waveform.to(self.device)\n",
        "    waveform = self._resample_if_necessary(waveform,sample_rate)\n",
        "    waveform = self._mix_down_if_necessary(waveform)\n",
        "    waveform = self._cut_if_necessary(waveform)\n",
        "    waveform = self._right_pad_if_necessary(waveform)\n",
        "    \n",
        "\n",
        "    if self.transformations:\n",
        "      for transformation in self.transformations:\n",
        "        waveform = transformation.to(self.device)(waveform)\n",
        "\n",
        "    # waveform normalisation:\n",
        "    waveform = torch.log1p(waveform)\n",
        "    waveform = waveform * 255/(waveform.max() -waveform.min())\n",
        "        \n",
        "    label = self.targets[idx]\n",
        "    return waveform, label\n",
        "\n",
        "  def _resample_if_necessary(self, waveform,sample_rate):\n",
        "    if sample_rate != self.target_sample_rate:\n",
        "      resampler = torchaudio.transforms.Resample(sample_rate,self.target_sample_rate)\n",
        "      waveform = resampler.to(self.device)(waveform)\n",
        "    return waveform\n",
        "\n",
        "  def _mix_down_if_necessary(self,waveform):\n",
        "    if waveform.shape[0] > 1:\n",
        "      waveform = torch.mean(waveform,dim = 0,keepdim = True)\n",
        "    return waveform\n",
        "\n",
        "  #If the video was longer than TARGET_LENGTH_SECONDS, then we are cropping it to that many seconds by removing seconds equally from both the start and the end sides\n",
        "  def _cut_if_necessary(self,waveform):\n",
        "    if waveform.shape[1] > self.num_samples:\n",
        "      mid = (waveform.shape[1] - 1)//2\n",
        "      nsby2 = self.num_samples//2\n",
        "      waveform = waveform[:,mid - nsby2 + 1: mid + self.num_samples - nsby2 + 1]\n",
        "    return waveform\n",
        "\n",
        "  def _right_pad_if_necessary(self,waveform):\n",
        "    num_samples = waveform.shape[1]\n",
        "    if num_samples < self.num_samples:\n",
        "      num_missing_samples = self.num_samples - num_samples\n",
        "      last_dim_padding = (0,num_missing_samples)\n",
        "      waveform = torch.nn.functional.pad(waveform,last_dim_padding)\n",
        "    return waveform\n",
        "\n",
        "\n",
        "\n",
        "# Define data directories\n",
        "train_dir: str\n",
        "val_dir:str\n",
        "if RunningInColab:\n",
        "    train_dir = \"/content/drive/MyDrive/audio_dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/audio_dataset/val\"\n",
        "else:\n",
        "    train_dir = \"audio_dataset/train\"\n",
        "    val_dir = \"audio_dataset/val\"\n",
        "\n",
        "train_dataset = AudioDataset(train_dir)\n",
        "val_dataset = AudioDataset(val_dir)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,batch_size = BATCH_SIZE, shuffle = True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,batch_size = BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhisim9Ncleb",
        "outputId": "a27c8c65-4387-4cc4-f299-8a223a52d423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 63]           3,136\n",
            "         Dropout2d-2           [-1, 64, 32, 63]               0\n",
            "       BatchNorm2d-3           [-1, 64, 32, 63]             128\n",
            "              ReLU-4           [-1, 64, 32, 63]               0\n",
            "         MaxPool2d-5           [-1, 64, 16, 32]               0\n",
            "            Conv2d-6           [-1, 64, 16, 32]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 16, 32]             128\n",
            "              ReLU-8           [-1, 64, 16, 32]               0\n",
            "            Conv2d-9           [-1, 64, 16, 32]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 16, 32]             128\n",
            "             ReLU-11           [-1, 64, 16, 32]               0\n",
            "       BasicBlock-12           [-1, 64, 16, 32]               0\n",
            "           Conv2d-13           [-1, 64, 16, 32]          36,864\n",
            "      BatchNorm2d-14           [-1, 64, 16, 32]             128\n",
            "             ReLU-15           [-1, 64, 16, 32]               0\n",
            "           Conv2d-16           [-1, 64, 16, 32]          36,864\n",
            "      BatchNorm2d-17           [-1, 64, 16, 32]             128\n",
            "             ReLU-18           [-1, 64, 16, 32]               0\n",
            "       BasicBlock-19           [-1, 64, 16, 32]               0\n",
            "        Dropout2d-20           [-1, 64, 16, 32]               0\n",
            "           Conv2d-21           [-1, 128, 8, 16]          73,728\n",
            "      BatchNorm2d-22           [-1, 128, 8, 16]             256\n",
            "             ReLU-23           [-1, 128, 8, 16]               0\n",
            "           Conv2d-24           [-1, 128, 8, 16]         147,456\n",
            "      BatchNorm2d-25           [-1, 128, 8, 16]             256\n",
            "           Conv2d-26           [-1, 128, 8, 16]           8,192\n",
            "      BatchNorm2d-27           [-1, 128, 8, 16]             256\n",
            "             ReLU-28           [-1, 128, 8, 16]               0\n",
            "       BasicBlock-29           [-1, 128, 8, 16]               0\n",
            "           Conv2d-30           [-1, 128, 8, 16]         147,456\n",
            "      BatchNorm2d-31           [-1, 128, 8, 16]             256\n",
            "             ReLU-32           [-1, 128, 8, 16]               0\n",
            "           Conv2d-33           [-1, 128, 8, 16]         147,456\n",
            "      BatchNorm2d-34           [-1, 128, 8, 16]             256\n",
            "             ReLU-35           [-1, 128, 8, 16]               0\n",
            "       BasicBlock-36           [-1, 128, 8, 16]               0\n",
            "        Dropout2d-37           [-1, 128, 8, 16]               0\n",
            "           Conv2d-38            [-1, 256, 4, 8]         294,912\n",
            "      BatchNorm2d-39            [-1, 256, 4, 8]             512\n",
            "             ReLU-40            [-1, 256, 4, 8]               0\n",
            "           Conv2d-41            [-1, 256, 4, 8]         589,824\n",
            "      BatchNorm2d-42            [-1, 256, 4, 8]             512\n",
            "           Conv2d-43            [-1, 256, 4, 8]          32,768\n",
            "      BatchNorm2d-44            [-1, 256, 4, 8]             512\n",
            "             ReLU-45            [-1, 256, 4, 8]               0\n",
            "       BasicBlock-46            [-1, 256, 4, 8]               0\n",
            "           Conv2d-47            [-1, 256, 4, 8]         589,824\n",
            "      BatchNorm2d-48            [-1, 256, 4, 8]             512\n",
            "             ReLU-49            [-1, 256, 4, 8]               0\n",
            "           Conv2d-50            [-1, 256, 4, 8]         589,824\n",
            "      BatchNorm2d-51            [-1, 256, 4, 8]             512\n",
            "             ReLU-52            [-1, 256, 4, 8]               0\n",
            "       BasicBlock-53            [-1, 256, 4, 8]               0\n",
            "        Dropout2d-54            [-1, 256, 4, 8]               0\n",
            "           Conv2d-55            [-1, 512, 2, 4]       1,179,648\n",
            "      BatchNorm2d-56            [-1, 512, 2, 4]           1,024\n",
            "             ReLU-57            [-1, 512, 2, 4]               0\n",
            "           Conv2d-58            [-1, 512, 2, 4]       2,359,296\n",
            "      BatchNorm2d-59            [-1, 512, 2, 4]           1,024\n",
            "           Conv2d-60            [-1, 512, 2, 4]         131,072\n",
            "      BatchNorm2d-61            [-1, 512, 2, 4]           1,024\n",
            "             ReLU-62            [-1, 512, 2, 4]               0\n",
            "       BasicBlock-63            [-1, 512, 2, 4]               0\n",
            "           Conv2d-64            [-1, 512, 2, 4]       2,359,296\n",
            "      BatchNorm2d-65            [-1, 512, 2, 4]           1,024\n",
            "             ReLU-66            [-1, 512, 2, 4]               0\n",
            "           Conv2d-67            [-1, 512, 2, 4]       2,359,296\n",
            "      BatchNorm2d-68            [-1, 512, 2, 4]           1,024\n",
            "             ReLU-69            [-1, 512, 2, 4]               0\n",
            "       BasicBlock-70            [-1, 512, 2, 4]               0\n",
            "        Dropout2d-71            [-1, 512, 2, 4]               0\n",
            "AdaptiveAvgPool2d-72            [-1, 512, 1, 1]               0\n",
            "           Linear-73                 [-1, 2048]       1,050,624\n",
            "           ResNet-74                 [-1, 2048]               0\n",
            "          Dropout-75                 [-1, 2048]               0\n",
            "           Linear-76                 [-1, 1024]       2,098,176\n",
            "           Linear-77                  [-1, 512]         524,800\n",
            "             ReLU-78                  [-1, 512]               0\n",
            "           Linear-79                   [-1, 13]           6,669\n",
            "================================================================\n",
            "Total params: 14,850,509\n",
            "Trainable params: 14,850,509\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 11.72\n",
            "Params size (MB): 56.65\n",
            "Estimated Total Size (MB): 68.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "\n",
        "# model = models.resnet50(weights = None).to(device)\n",
        "# model.conv1 = nn.Conv2d(1,64,kernel_size = 7,stride = 2,padding = 3,bias = False).to(device)\n",
        "\n",
        "# num_classes = len(class_mapping)\n",
        "# num_features = model.fc.in_features\n",
        "\n",
        "# model.fc = nn.Sequential(\n",
        "#     nn.Linear(num_features, num_classes),\n",
        "#     nn.Softmax(dim = 1)\n",
        "# ).to(device)\n",
        "\n",
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomResNet18, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=None).to(device)\n",
        "\n",
        "        self.resnet.conv1 = nn.Conv2d(1,64,kernel_size = 7,stride = 2,padding = 3,bias = False).to(device)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features,2048).to(device)\n",
        "        # self.replace_batchnorm_layers(self.resnet)\n",
        "        self.num_classes = len(class_mapping)\n",
        "\n",
        "        self.resnet.conv1 = nn.Sequential(\n",
        "            self.resnet.conv1,\n",
        "            nn.Dropout2d(DROPOUT_PROB_2D)  # Add dropout after the first convolutional layer\n",
        "        )\n",
        "        # Add dropout after layers in resnet\n",
        "        self.resnet.layer1 = nn.Sequential(\n",
        "            self.resnet.layer1,\n",
        "            nn.Dropout2d(DROPOUT_PROB_2D)  # Add dropout after layer1\n",
        "        )\n",
        "        self.resnet.layer2 = nn.Sequential(\n",
        "            self.resnet.layer2,\n",
        "            nn.Dropout2d(DROPOUT_PROB_2D)  # Add dropout after layer2\n",
        "        )\n",
        "        self.resnet.layer3 = nn.Sequential(\n",
        "            self.resnet.layer3,\n",
        "            nn.Dropout2d(DROPOUT_PROB_2D)  # Add dropout after layer3\n",
        "        )\n",
        "        self.resnet.layer4 = nn.Sequential(\n",
        "            self.resnet.layer4,\n",
        "            nn.Dropout2d(DROPOUT_PROB_2D)  # Add dropout after layer4\n",
        "        )\n",
        "\n",
        "        # Add additional linear layers\n",
        "        self.additional_layers = nn.ModuleList().to(device)\n",
        "        self.additional_layers.append(nn.Dropout(DROPOUT_PROB).to(device))\n",
        "        self.additional_layers.append(nn.Linear(2048, 1024).to(device))\n",
        "        self.additional_layers.append(nn.Linear(1024,512).to(device))\n",
        "        self.additional_layers.append(nn.ReLU().to(device))\n",
        "        self.additional_layers.append(nn.Linear(512,self.num_classes).to(device))\n",
        "    \n",
        "    # def replace_batchnorm_layers(self,model, replace_with=nn.Identity):\n",
        "    #     for name, module in model.named_children():\n",
        "    #         if isinstance(module, nn.BatchNorm2d):\n",
        "    #             setattr(model, name, replace_with())\n",
        "    #         else:\n",
        "    #             self.replace_batchnorm_layers(module, replace_with)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        for layer in self.additional_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "model = CustomResNet18()\n",
        "summary(model,(1,64,126))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X6oxjQvIeKv8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epoch):\n",
        "  model.train()  # Set model to training mode\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  batch_itr = 0\n",
        "  \n",
        "  for inputs, labels in train_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    # print(predicted)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    print(loss.item())\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    batch_itr+=1\n",
        "\n",
        "    print(f\"Batch {batch_itr} of {ceil(len(train_dataset)/BATCH_SIZE)}\")\n",
        "  epoch_loss = running_loss / len(train_loader.dataset)\n",
        "  print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
        "  train_accuracy = correct / total\n",
        "  print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Validation function\n",
        "def validate_model(model, val_loader):\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      # print(predicted)\n",
        "\n",
        "  val_accuracy = correct / total\n",
        "  print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "  return val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "yKN0MP8sek_r",
        "outputId": "f101817b-8f50-4a57-8518-b88d60ae2eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.057988740503787994\n",
            "Batch 1 of 38\n",
            "0.17925816774368286\n",
            "Batch 2 of 38\n",
            "0.07590349018573761\n",
            "Batch 3 of 38\n",
            "0.06926144659519196\n",
            "Batch 4 of 38\n",
            "0.05452224612236023\n",
            "Batch 5 of 38\n",
            "0.04531101882457733\n",
            "Batch 6 of 38\n",
            "0.13861696422100067\n",
            "Batch 7 of 38\n",
            "0.0883622020483017\n",
            "Batch 8 of 38\n",
            "0.012878292240202427\n",
            "Batch 9 of 38\n",
            "0.1098610907793045\n",
            "Batch 10 of 38\n",
            "0.06530620157718658\n",
            "Batch 11 of 38\n",
            "0.03174947574734688\n",
            "Batch 12 of 38\n",
            "0.10873483866453171\n",
            "Batch 13 of 38\n",
            "0.11338510364294052\n",
            "Batch 14 of 38\n",
            "0.026243267580866814\n",
            "Batch 15 of 38\n",
            "0.05841178447008133\n",
            "Batch 16 of 38\n",
            "0.0928352028131485\n",
            "Batch 17 of 38\n",
            "0.09884463995695114\n",
            "Batch 18 of 38\n",
            "0.13446098566055298\n",
            "Batch 19 of 38\n",
            "0.11314146220684052\n",
            "Batch 20 of 38\n",
            "0.014937084168195724\n",
            "Batch 21 of 38\n",
            "0.052648887038230896\n",
            "Batch 22 of 38\n",
            "0.2118903398513794\n",
            "Batch 23 of 38\n",
            "0.23520921170711517\n",
            "Batch 24 of 38\n",
            "0.04456484317779541\n",
            "Batch 25 of 38\n",
            "0.07342937588691711\n",
            "Batch 26 of 38\n",
            "0.0487155057489872\n",
            "Batch 27 of 38\n",
            "0.06649240851402283\n",
            "Batch 28 of 38\n",
            "0.07889424264431\n",
            "Batch 29 of 38\n",
            "0.07688266038894653\n",
            "Batch 30 of 38\n",
            "0.06491387635469437\n",
            "Batch 31 of 38\n",
            "0.07032063603401184\n",
            "Batch 32 of 38\n",
            "0.048705533146858215\n",
            "Batch 33 of 38\n",
            "0.09312754124403\n",
            "Batch 34 of 38\n",
            "0.027008192613720894\n",
            "Batch 35 of 38\n",
            "0.06037922948598862\n",
            "Batch 36 of 38\n",
            "0.03257729113101959\n",
            "Batch 37 of 38\n",
            "0.05274088680744171\n",
            "Batch 38 of 38\n",
            "Epoch [1/5], Loss: 0.0797\n",
            "Training Accuracy: 0.9776\n",
            "Validation Accuracy: 0.9107\n",
            "Best Validation Accuracy: 0.9107\n",
            "0.0515928640961647\n",
            "Batch 1 of 38\n",
            "0.09733038395643234\n",
            "Batch 2 of 38\n",
            "0.0891508162021637\n",
            "Batch 3 of 38\n",
            "0.06843975186347961\n",
            "Batch 4 of 38\n",
            "0.008164318278431892\n",
            "Batch 5 of 38\n",
            "0.015083864331245422\n",
            "Batch 6 of 38\n",
            "0.04379705712199211\n",
            "Batch 7 of 38\n",
            "0.023806607350707054\n",
            "Batch 8 of 38\n",
            "0.03558585047721863\n",
            "Batch 9 of 38\n",
            "0.06039771810173988\n",
            "Batch 10 of 38\n",
            "0.013365206308662891\n",
            "Batch 11 of 38\n",
            "0.018936755135655403\n",
            "Batch 12 of 38\n",
            "0.01545125711709261\n",
            "Batch 13 of 38\n",
            "0.05095312371850014\n",
            "Batch 14 of 38\n",
            "0.0018080079462379217\n",
            "Batch 15 of 38\n",
            "0.036688052117824554\n",
            "Batch 16 of 38\n",
            "0.06244464963674545\n",
            "Batch 17 of 38\n",
            "0.030263954773545265\n",
            "Batch 18 of 38\n",
            "0.08877860754728317\n",
            "Batch 19 of 38\n",
            "0.0999700203537941\n",
            "Batch 20 of 38\n",
            "0.03284342959523201\n",
            "Batch 21 of 38\n",
            "0.037486497312784195\n",
            "Batch 22 of 38\n",
            "0.0323704294860363\n",
            "Batch 23 of 38\n",
            "0.09933483600616455\n",
            "Batch 24 of 38\n",
            "0.027165640145540237\n",
            "Batch 25 of 38\n",
            "0.06706329435110092\n",
            "Batch 26 of 38\n",
            "0.016286272555589676\n",
            "Batch 27 of 38\n",
            "0.016338827088475227\n",
            "Batch 28 of 38\n",
            "0.025828534737229347\n",
            "Batch 29 of 38\n",
            "0.0943177118897438\n",
            "Batch 30 of 38\n",
            "0.016368774697184563\n",
            "Batch 31 of 38\n",
            "0.041007108986377716\n",
            "Batch 32 of 38\n",
            "0.07994115352630615\n",
            "Batch 33 of 38\n",
            "0.1388762891292572\n",
            "Batch 34 of 38\n",
            "0.02735094353556633\n",
            "Batch 35 of 38\n",
            "0.05307180434465408\n",
            "Batch 36 of 38\n",
            "0.039851922541856766\n",
            "Batch 37 of 38\n",
            "0.02624037303030491\n",
            "Batch 38 of 38\n",
            "Epoch [2/5], Loss: 0.0470\n",
            "Training Accuracy: 0.9846\n",
            "Validation Accuracy: 0.9041\n",
            "Best Validation Accuracy: 0.9107\n",
            "0.051949962973594666\n",
            "Batch 1 of 38\n",
            "0.053713228553533554\n",
            "Batch 2 of 38\n",
            "0.07001914829015732\n",
            "Batch 3 of 38\n",
            "0.1252092719078064\n",
            "Batch 4 of 38\n",
            "0.009581102058291435\n",
            "Batch 5 of 38\n",
            "0.040974099189043045\n",
            "Batch 6 of 38\n",
            "0.04344003275036812\n",
            "Batch 7 of 38\n",
            "0.11094918102025986\n",
            "Batch 8 of 38\n",
            "0.03631356358528137\n",
            "Batch 9 of 38\n",
            "0.014669104479253292\n",
            "Batch 10 of 38\n",
            "0.044122982770204544\n",
            "Batch 11 of 38\n",
            "0.010299206711351871\n",
            "Batch 12 of 38\n",
            "0.0890280082821846\n",
            "Batch 13 of 38\n",
            "0.013575519435107708\n",
            "Batch 14 of 38\n",
            "0.04853585362434387\n",
            "Batch 15 of 38\n",
            "0.07111611217260361\n",
            "Batch 16 of 38\n",
            "0.0219427440315485\n",
            "Batch 17 of 38\n",
            "0.043772824108600616\n",
            "Batch 18 of 38\n",
            "0.05406422168016434\n",
            "Batch 19 of 38\n",
            "0.04726447910070419\n",
            "Batch 20 of 38\n",
            "0.0244462788105011\n",
            "Batch 21 of 38\n",
            "0.05056748166680336\n",
            "Batch 22 of 38\n",
            "0.11266589164733887\n",
            "Batch 23 of 38\n",
            "0.09997259825468063\n",
            "Batch 24 of 38\n",
            "0.024702725932002068\n",
            "Batch 25 of 38\n",
            "0.014225710183382034\n",
            "Batch 26 of 38\n",
            "0.028281375765800476\n",
            "Batch 27 of 38\n",
            "0.016131997108459473\n",
            "Batch 28 of 38\n",
            "0.051441170275211334\n",
            "Batch 29 of 38\n",
            "0.035660434514284134\n",
            "Batch 30 of 38\n",
            "0.017326053231954575\n",
            "Batch 31 of 38\n",
            "0.0959111675620079\n",
            "Batch 32 of 38\n",
            "0.05951102823019028\n",
            "Batch 33 of 38\n",
            "0.025604547932744026\n",
            "Batch 34 of 38\n",
            "0.016637446358799934\n",
            "Batch 35 of 38\n",
            "0.08329421281814575\n",
            "Batch 36 of 38\n",
            "0.02607113867998123\n",
            "Batch 37 of 38\n",
            "0.010846279561519623\n",
            "Batch 38 of 38\n",
            "Epoch [3/5], Loss: 0.0473\n",
            "Training Accuracy: 0.9837\n",
            "Validation Accuracy: 0.9181\n",
            "Best Validation Accuracy: 0.9181\n",
            "0.006691882386803627\n",
            "Batch 1 of 38\n",
            "0.03658825904130936\n",
            "Batch 2 of 38\n",
            "0.027780678123235703\n",
            "Batch 3 of 38\n",
            "0.07714235782623291\n",
            "Batch 4 of 38\n",
            "0.035345081239938736\n",
            "Batch 5 of 38\n",
            "0.0273886751383543\n",
            "Batch 6 of 38\n",
            "0.10333345085382462\n",
            "Batch 7 of 38\n",
            "0.09581363201141357\n",
            "Batch 8 of 38\n",
            "0.04709915816783905\n",
            "Batch 9 of 38\n",
            "0.03359362855553627\n",
            "Batch 10 of 38\n",
            "0.04634774103760719\n",
            "Batch 11 of 38\n",
            "0.1146560087800026\n",
            "Batch 12 of 38\n",
            "0.004504156298935413\n",
            "Batch 13 of 38\n",
            "0.07776995003223419\n",
            "Batch 14 of 38\n",
            "0.06570764631032944\n",
            "Batch 15 of 38\n",
            "0.006313349585980177\n",
            "Batch 16 of 38\n",
            "0.04743601009249687\n",
            "Batch 17 of 38\n",
            "0.07395103573799133\n",
            "Batch 18 of 38\n",
            "0.19410696625709534\n",
            "Batch 19 of 38\n",
            "0.0514330230653286\n",
            "Batch 20 of 38\n",
            "0.18425557017326355\n",
            "Batch 21 of 38\n",
            "0.017092976719141006\n",
            "Batch 22 of 38\n",
            "0.052839841693639755\n",
            "Batch 23 of 38\n",
            "0.0374784916639328\n",
            "Batch 24 of 38\n",
            "0.03720660135149956\n",
            "Batch 25 of 38\n",
            "0.07529495656490326\n",
            "Batch 26 of 38\n",
            "0.054764457046985626\n",
            "Batch 27 of 38\n",
            "0.055301256477832794\n",
            "Batch 28 of 38\n",
            "0.1938418447971344\n",
            "Batch 29 of 38\n",
            "0.04365483298897743\n",
            "Batch 30 of 38\n",
            "0.07267624884843826\n",
            "Batch 31 of 38\n",
            "0.057726163417100906\n",
            "Batch 32 of 38\n",
            "0.1094946637749672\n",
            "Batch 33 of 38\n",
            "0.03994355723261833\n",
            "Batch 34 of 38\n",
            "0.01704975962638855\n",
            "Batch 35 of 38\n",
            "0.049299679696559906\n",
            "Batch 36 of 38\n",
            "0.05893712118268013\n",
            "Batch 37 of 38\n",
            "0.104124054312706\n",
            "Batch 38 of 38\n",
            "Epoch [4/5], Loss: 0.0640\n",
            "Training Accuracy: 0.9794\n",
            "Validation Accuracy: 0.9090\n",
            "Best Validation Accuracy: 0.9181\n",
            "0.011946795508265495\n",
            "Batch 1 of 38\n",
            "0.02170952782034874\n",
            "Batch 2 of 38\n",
            "0.05367576703429222\n",
            "Batch 3 of 38\n",
            "0.027460414916276932\n",
            "Batch 4 of 38\n",
            "0.029305316507816315\n",
            "Batch 5 of 38\n",
            "0.10313225537538528\n",
            "Batch 6 of 38\n",
            "0.03188531845808029\n",
            "Batch 7 of 38\n",
            "0.028228236362338066\n",
            "Batch 8 of 38\n",
            "0.04960138350725174\n",
            "Batch 9 of 38\n",
            "0.046885814517736435\n",
            "Batch 10 of 38\n",
            "0.05918280780315399\n",
            "Batch 11 of 38\n",
            "0.046013183891773224\n",
            "Batch 12 of 38\n",
            "0.02312525175511837\n",
            "Batch 13 of 38\n",
            "0.16027267277240753\n",
            "Batch 14 of 38\n",
            "0.017562393099069595\n",
            "Batch 15 of 38\n",
            "0.08843909204006195\n",
            "Batch 16 of 38\n",
            "0.02875462919473648\n",
            "Batch 17 of 38\n",
            "0.03174483776092529\n",
            "Batch 18 of 38\n",
            "0.020909538492560387\n",
            "Batch 19 of 38\n",
            "0.023842828348279\n",
            "Batch 20 of 38\n",
            "0.013883013278245926\n",
            "Batch 21 of 38\n",
            "0.028944917023181915\n",
            "Batch 22 of 38\n",
            "0.04104989022016525\n",
            "Batch 23 of 38\n",
            "0.10173289477825165\n",
            "Batch 24 of 38\n",
            "0.06072143465280533\n",
            "Batch 25 of 38\n",
            "0.04879714921116829\n",
            "Batch 26 of 38\n",
            "0.035224851220846176\n",
            "Batch 27 of 38\n",
            "0.05889641121029854\n",
            "Batch 28 of 38\n",
            "0.04427370801568031\n",
            "Batch 29 of 38\n",
            "0.07910814136266708\n",
            "Batch 30 of 38\n",
            "0.12700843811035156\n",
            "Batch 31 of 38\n",
            "0.041037704795598984\n",
            "Batch 32 of 38\n",
            "0.03737259656190872\n",
            "Batch 33 of 38\n",
            "0.024475930258631706\n",
            "Batch 34 of 38\n",
            "0.012608067132532597\n",
            "Batch 35 of 38\n",
            "0.04281734675168991\n",
            "Batch 36 of 38\n",
            "0.04557066783308983\n",
            "Batch 37 of 38\n",
            "0.03821650147438049\n",
            "Batch 38 of 38\n",
            "Epoch [5/5], Loss: 0.0470\n",
            "Training Accuracy: 0.9837\n",
            "Validation Accuracy: 0.9107\n",
            "Best Validation Accuracy: 0.9181\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_model(model,train_loader,criterion,optimizer,epoch)\n",
        "  accuracy = validate_model(model, val_loader)\n",
        "\n",
        "  if accuracy > best_accuracy:\n",
        "    best_accuracy = accuracy\n",
        "    # torch.save(model.state_dict(),'best_model.pth')\n",
        "\n",
        "  print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(),'epoch50_x_9107')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
